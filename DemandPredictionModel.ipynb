{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Prediction Model using Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_pickle('./DemandDataFile', compression='infer')\n",
    "df_region = pd.read_pickle('./RegionDataFile', compression='infer')\n",
    "df_initial = pd.merge(df_initial, df_region, how='inner', right_on=['CITY_NAME'], left_on=['CITY'])\n",
    "df_initial = df_initial.drop(['CITY_NAME'], axis=1)\n",
    "df_initial = df_initial[~df_initial['PRODUCT_NAME'].str.contains(\"Small Flyers|Large Flyers|Meter Bubble Wrap|Bundle of 50 Boxes\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.rename(columns = {'ORDER_DATE':'DATE'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.read_csv('./ProductWeights.csv')\n",
    "df_weights.drop_duplicates(subset=['COD_SKU_CONFIG'],inplace =True)\n",
    "\n",
    "df_productReviews = pd.read_csv('./ProductReviews.csv')\n",
    "df_productReviews.drop_duplicates(subset=['COD_SKU_CONFIG'],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.columns = ['SKU','PRODUCT_NAME','WEIGHT']\n",
    "df_initial = pd.merge(df_initial, df_weights[['SKU','WEIGHT']], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productReviews.columns = ['SKU','AVG_RATING']\n",
    "df_initial = pd.merge(df_initial, df_productReviews, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.loc[:,\"Voucher\"][df_initial.Voucher > 0] = True\n",
    "df_initial.loc[:,\"Voucher\"][df_initial.Voucher != True] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Month & WeekDay Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['WEEKDAY'] = df_initial['DATE'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "df_initial['MONTH'] = df_initial['DATE'].apply(lambda x:calendar.month_abbr[x.month])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_info = pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "display(tab_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing SKUs with least History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDays = 180 \n",
    "MinHistoryDays = int(totalDays * 0.02)\n",
    "#print(MinHistoryDays)\n",
    "temp = df_initial.groupby(['SKU'])['DATE'].count().to_frame('Count').reset_index()\n",
    "temp = temp[temp.Count >= MinHistoryDays]\n",
    "\n",
    "df_initial = pd.merge(df_initial, temp, how='inner')\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEIGHT Column Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_initial['WEIGHT'] = df_initial['WEIGHT'].apply(lambda x : re.sub(r'#', r'0', str(x)))\n",
    "df_initial['WEIGHT'] = df_initial['WEIGHT'].apply(lambda x : re.sub(r'[a-zA-Z]', '', str(x)))\n",
    "df_initial['WEIGHT'] = df_initial['WEIGHT'].apply(lambda x : re.sub(r'([.])\\1+', r'\\1', str(x)))\n",
    "df_initial['WEIGHT'] = df_initial['WEIGHT'].apply(lambda x : re.sub(r'/', r'', str(x)))\n",
    "df_initial['WEIGHT'] = df_initial['WEIGHT'].apply(lambda x : re.sub(r'[!]', '0', str(x)))\n",
    "df_initial['WEIGHT'] = pd.to_numeric(df_initial['WEIGHT'])\n",
    "weights_temp = df_initial['WEIGHT']\n",
    "\n",
    "df_initial.WEIGHT[df_initial['WEIGHT'].isnull()] = -1\n",
    "df_initial.WEIGHT[df_initial['WEIGHT'] >= 100] = df_initial.WEIGHT[df_initial['WEIGHT'] >= 100] / 1000\n",
    "len(df_initial.WEIGHT[df_initial['WEIGHT'] >= 100] / 1000)\n",
    "bins = [-2 , -1 ,0 , 10, 20, 30 ,40 ,50 ,60 ,70 ,df_initial['WEIGHT'].max()]\n",
    "labels = ['Unknown', 'Low < 0','Low (>0 <10)','Low (>10 <20)','Med (>20 <30)', \n",
    "          'Med (>30 <40)','Med (>40 <50)','Hi (>50 <60)','Hi (>60 <70)','Highest (>70)']\n",
    "df_initial['WEIGHT_BINNED'] = pd.cut(df_initial['WEIGHT'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIT PRICE Column Binning / Filtring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['PRICE_MEDIAN'] = df_initial.groupby('SKU')['UNIT_PRICE'].transform('median')\n",
    "bins = [1 , 15000 ,45000 , 100000 ,150000 ,df_initial['PRICE_MEDIAN'].max()]\n",
    "df_initial['PRICE_BINNED'] = pd.cut(df_initial['PRICE_MEDIAN'],bins=bins, labels=[\"low\",\"lower medium\", \"medium\",\"higher medium\", \"expensive\"])\n",
    "df_initial['PRICE_BINNED'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKU PRICE mapping check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_initial.groupby(['SKU'])['PRICE_MEDIAN'].nunique()  > 1\n",
    "if len(temp[temp == False]) != df_initial['SKU'].nunique():\n",
    "       print ('Some SKUs have duplicate prices, Should resolve this issue before resuming')\n",
    "else:\n",
    "       print('All SKUs in dataset have SINGLE price associated, we can resume')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promotions Dates Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_promotions = pd.read_csv('./PromotionDates.csv')\n",
    "\n",
    "df_promotions.StartDate = pd.to_datetime(df_promotions['StartDate'])\n",
    "df_promotions.EndDate = pd.to_datetime(df_promotions['EndDate'])\n",
    "df_promotions['Days'] = (df_promotions['EndDate'] - df_promotions['StartDate']).dt.days + 1\n",
    "#repeat rows\n",
    "df_promotions = df_promotions.loc[df_promotions.index.repeat(df_promotions['Days'])]\n",
    "#group by index with transform for date ranges\n",
    "df_promotions['Dates'] =(df_promotions.groupby(level=0)['StartDate']\n",
    "                         .transform(lambda x: pd.date_range(start=x.iat[0], periods=len(x))))\n",
    "#unique default index\n",
    "df_promotions = df_promotions.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region to Warehouse mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['WareHouse'] = 'Null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.loc[:,\"WareHouse\"][df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Karachi'\n",
    "df_initial.loc[:,\"WareHouse\"][~df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Lahore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['WareHouse'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame for ARIMA Time Series Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Model = df_initial[['SKU','PRODUCT_NAME','DATE','Quantity','WareHouse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Model = df_Model.groupby(by=['SKU','DATE'], as_index=False)['Quantity'].sum()\n",
    "df_Model.sort_values('DATE',ascending=True, inplace = True)\n",
    "df_Model.DATE = pd.to_datetime(df_Model['DATE'])\n",
    "df_Model = df_Model.set_index('DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TS Vis to check if Trend , Seasonality exists (i.e Sationary or Non- Stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTS = df_Model[df_Model.SKU == 'SH069FA039PJONAFAMZ'][['Quantity']]\n",
    "q = dataTS.Quantity.quantile(0.96)\n",
    "\n",
    "dataTS.Quantity[dataTS.Quantity > dataTS.Quantity.quantile(0.96)] = int(q)\n",
    "\n",
    "q = dataTS.Quantity.quantile(0.01)\n",
    "\n",
    "dataTS.Quantity[dataTS.Quantity < dataTS.Quantity.quantile(0.01)] = int(q)\n",
    "\n",
    "series = dataTS.copy()\n",
    "series.plot()\n",
    "series.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationary Test using Simple Mean / Varience measures , spliting dataset into 2 halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = series.values\n",
    "split = len(X) / 2\n",
    "X1, X2 = X[0:int(split)+1], X[int(split):]\n",
    "mean1, mean2 = X1.mean(), X2.mean()\n",
    "var1, var2 = X1.var(), X2.var()\n",
    "print('Mean1=%f, Mean2=%f' % (mean1, mean2))\n",
    "print('Variance1=%f, Variance2=%f' % (var1, var2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented Dickey-Fuller test for Stationary Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "X = series.values\n",
    "result = adfuller(pd.DataFrame(X).iloc[:,0])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "\n",
    "print('If ADF value is Negative and lesser then Critical values then TS is Stationary , else Non Stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.tools.plotting import autocorrelation_plot\n",
    "#autocorrelation_plot(series)\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(series,lags=40)\n",
    "print('')\n",
    "#series = np.array(series, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMIA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(series.values, order=(4,0,1))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "# plot residual errors\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "X = series.values\n",
    "size = int(len(X) * 0.85)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(2,0,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    #print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "\n",
    "tsModelErrors = {'ARIMA Model': math.sqrt(error)}\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.suptitle('Time Series Prediction for (Pack of 2 - Black & Grey Cotton Tshirts for Men) for 27 Days', fontsize= 16)\n",
    "plt.title('Test Unit Error: %.2f' % math.sqrt(error), fontsize=18)\n",
    "\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()\n",
    "\n",
    "#Pack of 2 - Black & Grey Cotton Tshirts for Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "\n",
    "train_size = int(len(dataTS) * 0.85)\n",
    "train, test = dataTS[0:train_size], dataTS[train_size:]\n",
    "    \n",
    "y_hat_avg = test.copy()\n",
    "\n",
    "fit1 = ExponentialSmoothing(train['Quantity'].ravel(),seasonal='add',seasonal_periods=11).fit(remove_bias = True,optimized = True)\n",
    "y_hat_avg['Holt_winter'] = fit1.forecast(len(test))\n",
    "rms = math.sqrt(mean_squared_error(test.Quantity, y_hat_avg.Holt_winter))\n",
    "tsModelErrors['Holt Winter'] = rms\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "#plt.plot(train['Quantity'], label='Train')\n",
    "plt.suptitle('Time Series Prediction for (Pack of 2 - Black & Grey Cotton Tshirts for Men) for 27 Days', fontsize= 16)\n",
    "plt.title('Test Unit Error: %.2f' % rms, fontsize=18)\n",
    "\n",
    "plt.plot(test['Quantity'], label='Test')\n",
    "plt.plot(y_hat_avg['Holt_winter'], label='Holt_winter')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#pd.concat([test.Quantity, y_hat_avg.Holt_winter], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "dataProphet = dataTS.reset_index()\n",
    "dataProphet.columns = ['ds','y']\n",
    "m = Prophet(changepoint_prior_scale=0.)\n",
    "m.fit(dataProphet);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=10)\n",
    "future.tail(10)\n",
    "\n",
    "forecast = m.predict(future)\n",
    "forecast['y'] = dataProphet.y\n",
    "forecast.rename(columns={'ds': 'Date','y': 'Actual Demand','yhat': 'Prediction','yhat_lower': 'Lower Bound','yhat_upper': 'Upper Bound'}, inplace=True)\n",
    "#forecast.columns = ['Date','Actual Demand','Prediction','Lower Bound','Upper Bound']\n",
    "forecast[['Date','Actual Demand','Prediction','Lower Bound','Upper Bound']][161:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.plot(forecast);\n",
    "rms = math.sqrt(mean_squared_error(forecast['Actual Demand'][154:180], forecast.Prediction[154:180]))\n",
    "tsModelErrors['Facebook Prophet Library'] = rms\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "#plt.plot(train['Quantity'], label='Train')\n",
    "plt.suptitle('Time Series Prediction using FACEBOOK library for (Pack of 2 - Black & Grey Cotton Tshirts for Men) for 27 Days', fontsize= 16)\n",
    "plt.title('Test Unit Error: %.2f' % rms, fontsize=18)\n",
    "\n",
    "plt.plot(forecast['Actual Demand'][154:180], label='Test')\n",
    "plt.plot(forecast['Prediction'][154:180], label='Predictions')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "plt.bar(range(len(tsModelErrors)), list(tsModelErrors.values()), align='center')\n",
    "plt.xticks(range(len(tsModelErrors)), list(tsModelErrors.keys()))\n",
    "plt.title('Error Comparision between the Time Series Models', fontsize=18)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = [\"Unit Error = %.2f\" % i for i in (tsModelErrors.values())]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
    "            ha='center', va='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "\t# prepare training dataset\n",
    "\ttrain_size = int(len(X) * 0.8)\n",
    "\ttrain, test = X[0:train_size], X[train_size:]\n",
    "\thistory = [x for x in train]\n",
    "\t# make predictions\n",
    "\tpredictions = list()\n",
    "\tfor t in range(len(test)):\n",
    "\t\tmodel = ARIMA(history, order=arima_order)\n",
    "\t\tmodel_fit = model.fit(disp=0)\n",
    "\t\tyhat = model_fit.forecast()[0]\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\thistory.append(test[t])\n",
    "\t# calculate out of sample error\n",
    "\terror = mean_squared_error(test, predictions)\n",
    "\treturn error\n",
    " \n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmse = evaluate_arima_model(dataset, order)\n",
    "\t\t\t\t\tif mse < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mse, order\n",
    "\t\t\t\t\tprint('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n",
    " \n",
    "\n",
    "# evaluate parameters\n",
    "p_values = [0, 1, 2, 4, 6, 8]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(series.values, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
