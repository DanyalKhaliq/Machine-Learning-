{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Prediction using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_pickle('./DemandDataFile', compression='infer')\n",
    "df_region = pd.read_pickle('./RegionDataFile', compression='infer')\n",
    "df_initial = pd.merge(df_initial, df_region, how='inner', right_on=['CITY_NAME'], left_on=['CITY'])\n",
    "df_initial = df_initial.drop(['CITY_NAME'], axis=1)\n",
    "\n",
    "df_initial = df_initial[~df_initial['PRODUCT_NAME'].str.contains(\"Small Flyers|Large Flyers|Meter Bubble Wrap|Bundle of 50 Boxes|Wrap\", na=False)]\n",
    "df_initial.rename(columns = {'ORDER_DATE':'DATE'},inplace = True)\n",
    "df_initial.sort_values('DATE',ascending=True, inplace = True)\n",
    "df_initial.DATE = pd.to_datetime(df_initial['DATE'])\n",
    "\n",
    "df_reviews = pd.read_csv('./ProductReviews.csv')\n",
    "df_initial = pd.merge(df_initial, df_reviews, how='left', right_on=['COD_SKU_CONFIG'], left_on=['SKU'])\n",
    "df_initial = df_initial.drop(['COD_SKU_CONFIG'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = pd.read_csv('./FradulentOrders.csv',dtype={'ORDER_NR': str})\n",
    "\n",
    "df_initial = df_initial[~df_initial.COD_ORDER_NR.isin(df_fraud.ORDER_NR.tolist())]\n",
    "\n",
    "df_initial['WareHouse'] = 'Null'\n",
    "df_initial.loc[:,\"WareHouse\"][df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Karachi'\n",
    "df_initial.loc[:,\"WareHouse\"][~df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Lahore'\n",
    "#df_initial['WareHouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHoliday(x):\n",
    "    if x in df_hday18.Date.values:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hday18 = pd.read_csv('./Holidays2018.csv')\n",
    "df_hday18.Date = pd.to_datetime(df_hday18['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_initial[df_initial.IsHoliday == 1].head()\n",
    "df_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['MedianPrice'] = df_initial.groupby('SKU')['UNIT_PRICE'].transform('median')\n",
    "df_initial['MedianPrice'] = pd.to_numeric(df_initial['MedianPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['CatConcat'] = df_initial[['PRODUCT_NAME','BRAND_NAME','CATEGORY_LEVEL_1', 'CATEGORY_LEVEL_2','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4']].apply(lambda x: ' | '.join(x.str.strip()), axis=1)\n",
    "df_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shift(df,dateCol,groupCol):\n",
    "    df['group_no'] = df.groupby([groupCol]).ngroup()\n",
    "    tmp = df[[dateCol,'Quantity','group_no']].set_index(['group_no',dateCol])\\\n",
    "                                          .unstack('group_no')\\\n",
    "                                          .resample('D').asfreq()\n",
    "    tmp1 = tmp.shift(1).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D1')\n",
    "    tmp2 = tmp.shift(2).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D2')\n",
    "    tmp3 = tmp.shift(3).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D3')\n",
    "    tmp4 = tmp.shift(4).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D4')\n",
    "    tmp5 = tmp.shift(5).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D5')\n",
    "    \n",
    "    df = df.join(tmp1, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp2, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp3, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp4, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp5, on=[dateCol,'group_no'])\n",
    "    \n",
    "    df.drop(axis=1, columns=['group_no'], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_inf(x):\n",
    "    if x>0:\n",
    "        return np.log(x) \n",
    "    else:\n",
    "        return np.log(1) \n",
    "\n",
    "def is_bundle(x):\n",
    "    if 'Bundle' in x or 'Pack' in x or '+' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_Gender(x):\n",
    "    if 'Men' in x:\n",
    "        return 'MEN'\n",
    "    elif 'Woman' in x:\n",
    "        return 'WOMAN'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "    \n",
    "def is_GroceryOrBaby(x):\n",
    "    if 'Grocer' in x or 'Baby' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def prepareDataFrame(wareHouse):\n",
    "    train_df = df_initial[['SKU','DATE','WareHouse','Quantity','MedianPrice','PRODUCT_NAME','CatConcat']][df_initial.WareHouse == wareHouse]\n",
    "    \n",
    "    \n",
    "    train_df['IsBundle'] = train_df['PRODUCT_NAME'].map(is_bundle)\n",
    "    train_df['ProductGender'] = train_df['CatConcat'].map(product_Gender)\n",
    "    train_df['IsGrocery'] = train_df['CatConcat'].map(is_GroceryOrBaby)\n",
    "    \n",
    "    #train_df = train_df[(train_df.SKU == 'HP770OT03D0JKNAFAMZ') | (train_df.SKU == 'SH069FA039PJONAFAMZ')]\n",
    "    train_df = train_df.groupby(by=['SKU','DATE','WareHouse','MedianPrice','IsBundle','IsGrocery','ProductGender','PRODUCT_NAME','CatConcat'], as_index=False)['Quantity'].sum()\n",
    "    train_df.sort_values('DATE',ascending=True, inplace = True)\n",
    "    train_df.DATE = pd.to_datetime(train_df['DATE'])\n",
    "    train_df = train_df.set_index('DATE')\n",
    "\n",
    "    \n",
    "    #Gettign the SKUs whcih were not demanded on the start date \n",
    "    startDate = '2017-12-01'\n",
    "    temp = train_df.reset_index().groupby('SKU').first()\n",
    "    temp.drop(temp[temp.DATE == startDate].index, inplace=True)\n",
    "\n",
    "    # replacing date to the Min Start date & Quantity Demand to None\n",
    "    temp['DATE'] = pd.to_datetime(startDate)\n",
    "    temp['Quantity'] = 0\n",
    "    if temp.index.name == 'SKU':\n",
    "        temp.reset_index(inplace = True)\n",
    "    \n",
    "    temp = temp.set_index('DATE')\n",
    "    \n",
    "    train_df = train_df.append(temp)\n",
    "    train_df.reset_index(inplace=True)\n",
    "    train_df['WEEKDAY'] = train_df['DATE'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    train_df['MONTH'] = train_df['DATE'].apply(lambda x:calendar.month_abbr[x.month])\n",
    "    train_df['IsHoliday'] = [isHoliday(x) for x in train_df['DATE'].values]\n",
    "    \n",
    "    train_df['YEAR'] = train_df['DATE'].apply(lambda x:x.year)\n",
    "    train_df['YEAR'] = train_df['YEAR'].apply(str)\n",
    "    \n",
    "    train_df['WEEKNO'] = train_df['DATE'].dt.week\n",
    "    #train_df['WEEKNO'] = train_df['WEEKNO'].apply(str)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    train_df = train_df.groupby(by=['SKU','YEAR','WEEKNO','WareHouse','MedianPrice','IsGrocery','IsBundle','ProductGender','PRODUCT_NAME','CatConcat'], as_index=False)['Quantity','IsHoliday'].sum()\n",
    "   \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = prepareDataFrame('Karachi')\n",
    "#temp.reset_index(inplace=True)\n",
    "#train_df = compute_shift(temp.copy(),'DATE','SKU')\n",
    "train_df = temp.copy()\n",
    "\n",
    "temp = prepareDataFrame('Lahore')\n",
    "#temp.reset_index(inplace=True)\n",
    "#temp = compute_shift(temp.copy(),'DATE','SKU')\n",
    "train_df = train_df.append(temp)\n",
    "train_df = train_df.sort_values(by=['YEAR', 'WEEKNO'], ascending=True)\n",
    "train_df['WEEKNO'] = train_df['WEEKNO'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Last_Week_Sales'] = train_df[train_df.WareHouse == 'Karachi'].groupby(['SKU'])['Quantity'].shift()[(train_df.WEEKNO == train_df.WEEKNO.shift() + 1)]\n",
    "#train_df['Last_Week_Diff'] = train_df.groupby(['SKU','WareHouse'])['Last_Week_Sales'].diff()[(train_df.WEEKNO == train_df.WEEKNO.shift() + 1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(0, inplace=True)\n",
    "train_df[(train_df.WareHouse == 'Karachi') & (train_df.SKU == '00301FA025DPKNAFAMZ')]\n",
    "#train_df[train_df.SKU == '00301FA025DPKNAFAMZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "#bins = np.arange(0,60,5) , use bins=bins in hist function below for smaller values\n",
    "train_df['Quantity'].hist(ax=ax, bottom=0.1)\n",
    "\n",
    "#formatter = FuncFormatter(log_10_product)\n",
    "ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[train_df['Quantity'] > 500].PRODUCT_NAME.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_initial.groupby(by=['DATE','SKU','CATEGORY_LEVEL_1'], as_index=False)['Quantity'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "g = sns.FacetGrid(test[test.Quantity < 1000], col=\"CATEGORY_LEVEL_1\")\n",
    "g = g.map(plt.hist, \"Quantity\", log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchMark Model (Predict Demand as Avergae of last N days demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = train_df[(train_df.DATE >= '2018-05-01') & (train_df.Quantity <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['PredictedDemand'] = np.int64((test_df.D1+test_df.D2+test_df.D3+test_df.D4+test_df.D5)/5)\n",
    "#test_df.loc[:,'PredictedDemand'] = test_df['PredictedDemand'].apply(lambda x : log_inf(x))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(\"MSE: \",mean_squared_error(test_df.Quantity, test_df.PredictedDemand),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error(test_df.Quantity, test_df.PredictedDemand))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.drop(axis=1, columns=['Karachi','Lahore'], inplace = True)\n",
    "if not {'MEN', 'NEUTRAL','Karachi','Lahore'}.issubset(train_df.columns):\n",
    "    dummyWareHouse = pd.get_dummies(train_df['WareHouse']).astype(int)\n",
    "    dummyProductGender = pd.get_dummies(train_df['ProductGender']).astype(int)\n",
    "    train_df = pd.concat([train_df,dummyWareHouse], axis = 1)\n",
    "    train_df = pd.concat([train_df,dummyProductGender], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['WEEKNO'] = train_df['WEEKNO'].apply(int)\n",
    "train_df[train_df.SKU == '00301FA025DPKNAFAMZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[(~train_df.WEEKNO.isin(testWeeks)) & (train_df.Quantity > 600)]['CatConcat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = ['WEEKNO','Lahore','IsHoliday','IsGrocery','IsBundle','MEN','NEUTRAL','MedianPrice']\n",
    "testWeeks = [19,20,21,22]\n",
    "X = train_df[~train_df.WEEKNO.isin(testWeeks)][colList]\n",
    "#X.MedianPrice = X.MedianPrice.map(log_inf)\n",
    "Y_orig = train_df[~train_df.WEEKNO.isin(testWeeks)][['Quantity']]\n",
    "#Y = Y_orig.Quantity.map(log_inf)\n",
    "Y = Y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df[train_df.WEEKNO.isin(testWeeks)][colList]\n",
    "#X_test.MedianPrice = X_test.MedianPrice.map(log_inf)\n",
    "Y_test_orig = train_df[train_df.WEEKNO.isin(testWeeks)][['Quantity']]\n",
    "#Y_test = Y_test_orig.Quantity.map(log_inf)\n",
    "Y_test = Y_test_orig\n",
    "#X_test_SKUs = train_df[(train_df.DATE >= '2018-05-01') & (train_df.Quantity <= 200)][['SKU']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Decision Tree Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr = DecisionTreeRegressor(max_depth=6,min_samples_split=10,min_samples_leaf=10)\n",
    "regr.fit(X, Y)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Actual'] = Y_test\n",
    "X_test['Predicted'] = y_pred\n",
    "#X_test['Actual'] = X_test['Actual'].map(np.exp)\n",
    "#X_test['Predicted'] = X_test['Predicted'].map(np.exp)\n",
    "#X_test[X_test.Actual > 5].head(100)\n",
    "#X.MedianPrice.describe()\n",
    "print('Total Actual Demand : ',X_test.Actual.sum(),'\\nTotal Predicted Demand : ',X_test.Predicted.sum())\n",
    "\n",
    "X_test[X_test.Actual >1][['Actual','Predicted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['WeekDayNo','Lahore','IsBundle','MEN','NEUTRAL']\n",
    "visualize_tree(regr, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_list = list(set(X_test.SKU.unique())-set(X.SKU.unique()))\n",
    "len(main_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor  \n",
    "import scipy.stats as st\n",
    "\n",
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(1, 20)\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 10),\n",
    "    \"max_depth\": st.randint(3, 10),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor(n_estimators=10) \n",
    "\n",
    "xgbreg.fit(X, Y)\n",
    "y_pred = xgbreg.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a Linear Regressor   \n",
    "lin_regressor = LinearRegression()\n",
    "\n",
    "# pass the order of your polynomial here  \n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "# convert to be used further to linear regression\n",
    "X_transform = poly.fit_transform(X)\n",
    "\n",
    "# fit this to Linear Regressor\n",
    "lin_regressor.fit(X_transform,Y.Quantity) \n",
    "\n",
    "# get the predictions\n",
    "y_pred = lin_regressor.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = X_test\n",
    "temp['Pred'] = y_pred\n",
    "temp['Act'] = Y_test\n",
    "\n",
    "temp[temp.Act >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch For Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gs = RandomizedSearchCV(xgbreg, params, n_jobs=1)  \n",
    "gs.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
