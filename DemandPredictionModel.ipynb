{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Prediction using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error , r2_score\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_pickle('./DemandDataFile', compression='infer')\n",
    "df_region = pd.read_pickle('./RegionDataFile', compression='infer')\n",
    "df_initial = pd.merge(df_initial, df_region, how='inner', right_on=['CITY_NAME'], left_on=['CITY'])\n",
    "df_initial = df_initial.drop(['CITY_NAME'], axis=1)\n",
    "\n",
    "df_initial = df_initial[~df_initial['PRODUCT_NAME'].str.contains(\"Small Flyers|Large Flyers|Meter Bubble Wrap|Bundle of 50 Boxes|Wrap\", na=False)]\n",
    "df_initial.rename(columns = {'ORDER_DATE':'DATE'},inplace = True)\n",
    "df_initial.sort_values('DATE',ascending=True, inplace = True)\n",
    "df_initial.DATE = pd.to_datetime(df_initial['DATE'])\n",
    "\n",
    "df_reviews = pd.read_csv('./ProductReviews.csv')\n",
    "df_initial = pd.merge(df_initial, df_reviews, how='left', right_on=['COD_SKU_CONFIG'], left_on=['SKU'])\n",
    "df_initial = df_initial.drop(['COD_SKU_CONFIG'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = pd.read_csv('./FradulentOrders.csv',dtype={'ORDER_NR': str})\n",
    "\n",
    "df_initial = df_initial[~df_initial.COD_ORDER_NR.isin(df_fraud.ORDER_NR.tolist())]\n",
    "\n",
    "df_initial['WareHouse'] = 'Null'\n",
    "df_initial.loc[:,\"WareHouse\"][df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Karachi'\n",
    "df_initial.loc[:,\"WareHouse\"][~df_initial['REGION_NAME'].isin(['Sindh','Balochistan'])] = 'Lahore'\n",
    "#df_initial['WareHouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHoliday(x):\n",
    "    if x in df_hday18.Date.values:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hday18 = pd.read_csv('./Holidays2018.csv')\n",
    "df_hday18.Date = pd.to_datetime(df_hday18['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_initial[df_initial.IsHoliday == 1].head()\n",
    "df_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['MedianPrice'] = df_initial.groupby('SKU')['UNIT_PRICE'].transform('median')\n",
    "df_initial['MedianPrice'] = pd.to_numeric(df_initial['MedianPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['CatConcat'] = df_initial[['PRODUCT_NAME','BRAND_NAME','CATEGORY_LEVEL_1', 'CATEGORY_LEVEL_2','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4']].apply(lambda x: ' | '.join(x.str.strip()), axis=1)\n",
    "df_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shift(df,dateCol,groupCol):\n",
    "    df['group_no'] = df.groupby([groupCol]).ngroup()\n",
    "    tmp = df[[dateCol,'Quantity','group_no']].set_index(['group_no',dateCol])\\\n",
    "                                          .unstack('group_no')\\\n",
    "                                          .resample('D').asfreq()\n",
    "    tmp1 = tmp.shift(1).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D1')\n",
    "    tmp2 = tmp.shift(2).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D2')\n",
    "    tmp3 = tmp.shift(3).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D3')\n",
    "    tmp4 = tmp.shift(4).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D4')\n",
    "    tmp5 = tmp.shift(5).fillna(0).astype(int).stack('group_no')['Quantity'].rename('D5')\n",
    "    \n",
    "    df = df.join(tmp1, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp2, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp3, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp4, on=[dateCol,'group_no'])\n",
    "    df = df.join(tmp5, on=[dateCol,'group_no'])\n",
    "    \n",
    "    df.drop(axis=1, columns=['group_no'], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_inf(x):\n",
    "    if x>0:\n",
    "        return np.log1p(x) \n",
    "    else:\n",
    "        return np.log1p(0) \n",
    "\n",
    "def is_bundle(x):\n",
    "    if 'Bundle' in x or 'Pack' in x or '+' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_Gender(x):\n",
    "    if 'Men' in x:\n",
    "        return 'MEN'\n",
    "    elif 'Woman' in x or 'Jewellery' in x:\n",
    "        return 'WOMAN'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "    \n",
    "def is_Grocery(x):\n",
    "    if 'Grocer' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_Baby(x):\n",
    "    if 'Baby' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "def is_PrevWeekHoliday(x):\n",
    "    if len(tempHolidayWeek[tempHolidayWeek  == x].values) > 0 :\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def prepareDataFrame(wareHouse):\n",
    "    train_df = df_initial[['SKU','DATE','WareHouse','Quantity','MedianPrice','PRODUCT_NAME','CatConcat','CATEGORY_LEVEL_1','CATEGORY_LEVEL_2','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME']][df_initial.WareHouse == wareHouse]\n",
    "    \n",
    "    \n",
    "    train_df['IsBundle'] = train_df['PRODUCT_NAME'].map(is_bundle)\n",
    "    train_df['ProductGender'] = train_df['CatConcat'].map(product_Gender)\n",
    "    train_df['IsGrocery'] = train_df['CatConcat'].map(is_Grocery)\n",
    "    train_df['IsBaby'] = train_df['CatConcat'].map(is_Baby)\n",
    "    \n",
    "    #train_df = train_df[(train_df.SKU == 'HP770OT03D0JKNAFAMZ') | (train_df.SKU == 'SH069FA039PJONAFAMZ')]\n",
    "    train_df = train_df.groupby(by=['SKU','DATE','WareHouse','MedianPrice','IsBundle','IsGrocery','IsBaby','ProductGender','PRODUCT_NAME','CatConcat','CATEGORY_LEVEL_1','CATEGORY_LEVEL_2','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME'], as_index=False)['Quantity'].sum()\n",
    "    train_df.sort_values('DATE',ascending=True, inplace = True)\n",
    "    train_df.DATE = pd.to_datetime(train_df['DATE'])\n",
    "    train_df = train_df.set_index('DATE')\n",
    "\n",
    "    \n",
    "    #Gettign the SKUs whcih were not demanded on the start date \n",
    "    startDate = '2017-12-01'\n",
    "    temp = train_df.reset_index().groupby('SKU').first()\n",
    "    temp.drop(temp[temp.DATE == startDate].index, inplace=True)\n",
    "\n",
    "    # replacing date to the Min Start date & Quantity Demand to None\n",
    "    temp['DATE'] = pd.to_datetime(startDate)\n",
    "    temp['Quantity'] = 0\n",
    "    if temp.index.name == 'SKU':\n",
    "        temp.reset_index(inplace = True)\n",
    "    \n",
    "    temp = temp.set_index('DATE')\n",
    "    \n",
    "    train_df = train_df.append(temp)\n",
    "    train_df.reset_index(inplace=True)\n",
    "    train_df['WEEKDAY'] = train_df['DATE'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    train_df['MONTH'] = train_df['DATE'].apply(lambda x:calendar.month_abbr[x.month])\n",
    "    train_df['IsHoliday'] = [isHoliday(x) for x in train_df['DATE'].values]\n",
    "    \n",
    "    train_df['YEAR'] = train_df['DATE'].apply(lambda x:x.year)\n",
    "    train_df['YEAR'] = train_df['YEAR'].apply(str)\n",
    "    \n",
    "    train_df['WEEKNO'] = train_df['DATE'].dt.week\n",
    "    #train_df['WEEKNO'] = train_df['WEEKNO'].apply(str)\n",
    "        \n",
    "    \n",
    "   \n",
    "    train_df = train_df.groupby(by=['SKU','YEAR','WEEKNO','WareHouse','MedianPrice','IsGrocery','IsBaby','IsBundle','ProductGender','PRODUCT_NAME','CatConcat','CATEGORY_LEVEL_1','CATEGORY_LEVEL_2','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME'], as_index=False)['Quantity','IsHoliday'].sum()\n",
    "   \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempKhi = prepareDataFrame('Karachi')\n",
    "tempKhi = tempKhi.sort_values(by=['SKU','YEAR', 'WEEKNO'], ascending=True)\n",
    "tempKhi['WEEKNO'] = tempKhi['WEEKNO'].apply(int)\n",
    "tempKhi.fillna(0, inplace=True)\n",
    "print(tempKhi.shape)\n",
    "#train_df = temp.copy()\n",
    "\n",
    "tempLhr = prepareDataFrame('Lahore')\n",
    "tempLhr = tempLhr.sort_values(by=['SKU','YEAR', 'WEEKNO'], ascending=True)\n",
    "tempLhr['WEEKNO'] = tempLhr['WEEKNO'].apply(int)\n",
    "tempLhr.fillna(0, inplace=True)\n",
    "print(tempLhr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempLhr['Last_Week_Sales'] = tempLhr.groupby(['SKU'])['Quantity'].shift()[(tempLhr.WEEKNO == tempLhr.WEEKNO.shift() + 1)]\n",
    "tempLhr['Last_Week_Diff'] = tempLhr.groupby(['SKU'])['Last_Week_Sales'].diff(1)[(tempLhr.WEEKNO == tempLhr.WEEKNO.shift() + 1)]\n",
    "\n",
    "tempLhr['Last_2Week_Sales'] = tempLhr.groupby(['SKU'])['Quantity'].shift(2)[(tempLhr.WEEKNO == tempLhr.WEEKNO.shift(1) + 1)]\n",
    "#tempLhr['Last_2Week_Diff'] = tempLhr.groupby(['SKU'])['Last_2Week_Sales'].diff()[(tempLhr.WEEKNO == tempLhr.WEEKNO.shift(2) + 1)]\n",
    "\n",
    "#tempLhr[(tempLhr.SKU == '00301FA025DPKNAFAMZ')][['YEAR','WEEKNO','Quantity','Last_Week_Sales','Last_2Week_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempKhi['Last_Week_Sales'] = tempKhi.groupby(['SKU'])['Quantity'].shift()[(tempKhi.WEEKNO == tempKhi.WEEKNO.shift() + 1)]\n",
    "tempKhi['Last_Week_Diff'] = tempKhi.groupby(['SKU'])['Last_Week_Sales'].diff()[(tempKhi.WEEKNO == tempKhi.WEEKNO.shift() + 1)]\n",
    "\n",
    "tempKhi['Last_2Week_Sales'] = tempKhi.groupby(['SKU'])['Quantity'].shift(2)[(tempKhi.WEEKNO == tempKhi.WEEKNO.shift(1) + 1)]\n",
    "#tempKhi['Last_2Week_Diff'] = tempKhi.groupby(['SKU'])['Last_2Week_Sales'].diff()[(tempKhi.WEEKNO == tempKhi.WEEKNO.shift(2) + 1)]\n",
    "\n",
    "#tempKhi[(tempKhi.SKU == '00301FA025DPKNAFAMZ')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([tempKhi, tempLhr])\n",
    "tempHolidayWeek = temp[temp.IsHoliday == 1]['WEEKNO'].unique()\n",
    "\n",
    "tempHolidayWeek = (pd.Series(tempHolidayWeek + 1))\n",
    "temp['PrevWeekHoliday'] = temp.WEEKNO.apply(lambda x : len(tempHolidayWeek[tempHolidayWeek  == x].values) > 0)\n",
    "\n",
    "#since we added 1 above so we are subtracting 2\n",
    "tempHolidayWeek = (pd.Series(tempHolidayWeek - 2))\n",
    "temp['NextWeekHoliday'] = temp.WEEKNO.apply(lambda x : len(tempHolidayWeek[tempHolidayWeek  == x].values) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.fillna(0, inplace=True)\n",
    "#temp[temp.SKU == '00301FA025DPKNAFAMZ']\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[train_df['Quantity'] > 500].PRODUCT_NAME.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_initial.groupby(by=['DATE','SKU','CATEGORY_LEVEL_1'], as_index=False)['Quantity'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "g = sns.FacetGrid(test[test.Quantity < 1000], col=\"CATEGORY_LEVEL_1\")\n",
    "g = g.map(plt.hist, \"Quantity\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "#bins = np.arange(0,60,5) , use bins=bins in hist function below for smaller values\n",
    "pd.DataFrame(Y).hist(ax=ax, bottom=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchMark Model (Predict Demand as Avergae of last N days demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = train_df[(train_df.DATE >= '2018-05-01') & (train_df.Quantity <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['PredictedDemand'] = np.int64((test_df.D1+test_df.D2+test_df.D3+test_df.D4+test_df.D5)/5)\n",
    "#test_df.loc[:,'PredictedDemand'] = test_df['PredictedDemand'].apply(lambda x : log_inf(x))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(\"MSE: \",mean_squared_error(test_df.Quantity, test_df.PredictedDemand),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error(test_df.Quantity, test_df.PredictedDemand))\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.drop(axis=1, columns=['Karachi','Lahore'], inplace = True)\n",
    "if not {'MEN', 'NEUTRAL','Karachi','Lahore'}.issubset(temp.columns):\n",
    "    dummyWareHouse = pd.get_dummies(temp['WareHouse']).astype(int)\n",
    "    dummyProductGender = pd.get_dummies(temp['ProductGender']).astype(int)\n",
    "    temp = pd.concat([temp,dummyWareHouse], axis = 1)\n",
    "    temp = pd.concat([temp,dummyProductGender], axis = 1)\n",
    "    temp.PrevWeekHoliday = temp.PrevWeekHoliday.astype(int)\n",
    "    temp.NextWeekHoliday = temp.NextWeekHoliday.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEKNO</th>\n",
       "      <th>WareHouse</th>\n",
       "      <th>MedianPrice</th>\n",
       "      <th>IsGrocery</th>\n",
       "      <th>IsBaby</th>\n",
       "      <th>IsBundle</th>\n",
       "      <th>ProductGender</th>\n",
       "      <th>PRODUCT_NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>Last_Week_Sales</th>\n",
       "      <th>Last_Week_Diff</th>\n",
       "      <th>Last_2Week_Sales</th>\n",
       "      <th>PrevWeekHoliday</th>\n",
       "      <th>NextWeekHoliday</th>\n",
       "      <th>Karachi</th>\n",
       "      <th>Lahore</th>\n",
       "      <th>MEN</th>\n",
       "      <th>NEUTRAL</th>\n",
       "      <th>WOMAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00301FA025DPKNAFAMZ</td>\n",
       "      <td>2017</td>\n",
       "      <td>48</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEN</td>\n",
       "      <td>Unisex Style Baseball Cap - Black</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00301FA025DPKNAFAMZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>18</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEN</td>\n",
       "      <td>Unisex Style Baseball Cap - Black</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00301FA025DPKNAFAMZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>19</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEN</td>\n",
       "      <td>Unisex Style Baseball Cap - Black</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00301FA025DPKNAFAMZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>21</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEN</td>\n",
       "      <td>Unisex Style Baseball Cap - Black</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00301FA0QSN4YNAFAMZ</td>\n",
       "      <td>2017</td>\n",
       "      <td>48</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Blue Golden Tulip Brooch For Women</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   SKU  YEAR  WEEKNO WareHouse  MedianPrice  IsGrocery  \\\n",
       "0  00301FA025DPKNAFAMZ  2017      48   Karachi        399.0          0   \n",
       "1  00301FA025DPKNAFAMZ  2018      18   Karachi        399.0          0   \n",
       "2  00301FA025DPKNAFAMZ  2018      19   Karachi        399.0          0   \n",
       "3  00301FA025DPKNAFAMZ  2018      21   Karachi        399.0          0   \n",
       "4  00301FA0QSN4YNAFAMZ  2017      48   Karachi        600.0          0   \n",
       "\n",
       "   IsBaby  IsBundle ProductGender                        PRODUCT_NAME  ...    \\\n",
       "0       0         0           MEN   Unisex Style Baseball Cap - Black  ...     \n",
       "1       0         0           MEN   Unisex Style Baseball Cap - Black  ...     \n",
       "2       0         0           MEN   Unisex Style Baseball Cap - Black  ...     \n",
       "3       0         0           MEN   Unisex Style Baseball Cap - Black  ...     \n",
       "4       0         0       NEUTRAL  Blue Golden Tulip Brooch For Women  ...     \n",
       "\n",
       "  Last_Week_Sales Last_Week_Diff Last_2Week_Sales PrevWeekHoliday  \\\n",
       "0             0.0            0.0              0.0               0   \n",
       "1             0.0            0.0              0.0               0   \n",
       "2             1.0            0.0              0.0               1   \n",
       "3             0.0            0.0              0.0               0   \n",
       "4             0.0            0.0              0.0               0   \n",
       "\n",
       "  NextWeekHoliday Karachi  Lahore  MEN  NEUTRAL  WOMAN  \n",
       "0               0       1       0    1        0      0  \n",
       "1               0       1       0    1        0      0  \n",
       "2               0       1       0    1        0      0  \n",
       "3               0       1       0    1        0      0  \n",
       "4               0       1       0    0        1      0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temp.to_pickle('./DemandForecastData',compression='infer', protocol=4)\n",
    "temp = pd.read_pickle('./DemandForecastData', compression='infer')\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandTable = pd.DataFrame(temp.BRAND_NAME.value_counts())\n",
    "brandTable[brandTable.BRAND_NAME > np.percentile(brandTable.BRAND_NAME,90)].sum()\n",
    "\n",
    "#np.percentile(brandTable.BRAND_NAME,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835047, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodIds = pd.DataFrame(temp.SKU.unique()).reset_index()\n",
    "prodIds.rename(columns = {'index':'ProdId',0:'SKU'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = ['WEEKNO','SKU','Lahore','NEUTRAL','IsHoliday',\n",
    "           \n",
    "           'Last_Week_Sales','Last_2Week_Sales','MedianPrice',\n",
    "           'BRAND_NAME','Quantity']\n",
    "toExcludeCols = ['SKU','Quantity','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME']\n",
    "\n",
    "toExcludeWeeks = [48,49,50,51,52]\n",
    "testWeeks = [19,20,21,22]\n",
    "#train_df = temp[temp.Quantity <= np.percentile(train_df.Quantity,99)]\n",
    "\n",
    "train_df = temp.copy()\n",
    "#train_df = train_df[train_df.Lahore == 1]\n",
    "#train_df.loc[train_df.IsGrocery > 0,'IsGrocery'] = 100\n",
    "train_df['%UpDown'] = (((train_df.Last_Week_Sales + 1) - (train_df.Last_2Week_Sales + 1)) / (train_df.Last_2Week_Sales + 1)) * 100\n",
    "train_df.loc[train_df.MedianPrice > np.percentile(train_df.MedianPrice,95) ,'MedianPrice'] = np.percentile(train_df.MedianPrice,95)\n",
    "train_df.loc[train_df.Quantity >= np.percentile(train_df.Quantity,99.9),'Quantity'] = np.percentile(train_df.Quantity,99.9)\n",
    "\n",
    "X = train_df[(~train_df.WEEKNO.isin(testWeeks) & ~train_df.WEEKNO.isin(toExcludeWeeks))][colList].copy()\n",
    "\n",
    "#X.MedianPrice = X.MedianPrice.map(log_inf)\n",
    "#Y_orig = train_df[~train_df.WEEKNO.isin(testWeeks)][['Quantity']]\n",
    "#Y = Y_orig.Quantity.map(log_inf)\n",
    "\n",
    "#Adding Avg Quantity DEmanded during the period of study\n",
    "\n",
    "X['LastDemand'] = X.groupby(by=['SKU','Lahore'])['Quantity'].transform('last')\n",
    "\n",
    "#X['LastCat3Demand'] = X[['CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME']].apply(lambda x: ' | '.join(x.str.strip()), axis=1)\n",
    "#X['LastCat3Demand'] = X.groupby(by=['Lahore','LastCat3Demand'])['Quantity'].transform('mean')\n",
    "\n",
    "\n",
    "# tt = X[['WEEKNO','SKU','Lahore','Quantity']]\n",
    "# tt['RollAvg']  = tt.groupby(by=['SKU','Lahore'])['Quantity'].rolling(3).mean().reset_index()['Quantity'].values\n",
    "\n",
    "# col_to_use = tt.columns.difference(X.columns)\n",
    "# X = pd.merge(X,tt[col_to_use], how='inner', right_on=['SKU','WEEKNO','Lahore'], left_on=['SKU','WEEKNO','Lahore'])\n",
    "#X = pd.merge(X,prodIds, how='inner', right_on=['SKU'], left_on=['SKU'])\n",
    "\n",
    "#X[X.SKU == '00301FA025DPKNAFAMZ'].head(100)\n",
    "#X[X.Lahore == 1].SKU.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from training sample to add average demand into test data with no data leakage\n",
    "tempAvgDemand = X[['SKU','Lahore','LastDemand']].drop_duplicates().copy()\n",
    "#tempProdId = X[['SKU','ProdId']].drop_duplicates().copy()\n",
    "\n",
    "X_test = train_df[train_df.WEEKNO.isin(testWeeks)][colList]\n",
    "\n",
    "X_test = pd.merge(X_test,tempAvgDemand, how='inner', right_on=['SKU','Lahore'], left_on=['SKU','Lahore'])\n",
    "\n",
    "#X_test.MedianPrice = X_test.MedianPrice.map(log_inf)\n",
    "#Y_test_orig = train_df[train_df.WEEKNO.isin(testWeeks)][['Quantity']]\n",
    "#Y_test = Y_test_orig.Quantity.map(log_inf)\n",
    "\n",
    "#X_test = pd.merge(X_test,tempProdId, how='inner', right_on=['SKU'], left_on=['SKU'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING \n",
    "scaleColList = ['MedianPrice','LastDemand','LastCat3Demand','Last_Week_Sales','Last_2Week_Sales','LastDemand','%UpDown']\n",
    "\n",
    "scalerX = preprocessing.MinMaxScaler().fit(X[scaleColList])\n",
    "scalerY = preprocessing.MinMaxScaler().fit(X['Quantity'].values.reshape(-1, 1))\n",
    "X[scaleColList] = scalerX.transform(X[scaleColList])\n",
    "Y = scalerY.transform(X['Quantity'].values.reshape(-1, 1))\n",
    "X_test[scaleColList] = scalerX.transform(X_test[scaleColList])\n",
    "Y_test = scalerY.transform(X_test['Quantity'].values.reshape(-1, 1))\n",
    "\n",
    "#X_test = X_test[X_test.columns.difference(['Quantity','SKU','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4'])]\n",
    "#X = X[X.columns.difference(['SKU','Quantity','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[scaleColList] = X[scaleColList].applymap(log_inf)\n",
    "X_test[scaleColList] = X_test[scaleColList].applymap(log_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tX\n",
    "# X_test = tXs\n",
    "#tX = X.copy()\n",
    "#tXs = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[X.Lahore == 0]\n",
    "# X_test = X_test[X_test.Lahore == 0]\n",
    "\n",
    "\n",
    "Y = X.loc[:,'Quantity']#.map(log_inf)\n",
    "Y_test = X_test.loc[:,'Quantity']#.map(log_inf).copy()\n",
    "\n",
    "\n",
    "#REMOVE Not Required Columns from both Train/Test Sets\n",
    "\n",
    "X_test = X_test[X_test.columns.difference(['Quantity','SKU','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME'])]\n",
    "X = X[X.columns.difference(['SKU','Quantity','CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = Y_test\n",
    "#Y_test = temp\n",
    "\n",
    "\n",
    "X =      X[X.columns.difference(['SKU','PrevWeekHoliday','NextWeekHoliday','LastDemand'\n",
    "           , 'IsBaby', 'IsBundle','IsGrocery','%UpDown'])]\n",
    "X_test = X_test[X_test.columns.difference(['SKU','PrevWeekHoliday','NextWeekHoliday','LastDemand'\n",
    "           , 'IsBaby', 'IsBundle','IsGrocery','%UpDown'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsHoliday             int64\n",
       "Lahore                int64\n",
       "Last_2Week_Sales    float64\n",
       "Last_Week_Sales     float64\n",
       "MedianPrice         float64\n",
       "NEUTRAL               int64\n",
       "WEEKNO                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  47.88585999495875 RMSE:  6.919960982184708 R2: 0.36449567918707826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr = DecisionTreeRegressor(criterion='mse', max_depth=14, max_features='auto',\n",
    "           max_leaf_nodes=30, min_impurity_decrease=0,\n",
    "           min_impurity_split=None, min_samples_leaf=15,\n",
    "           min_samples_split=12, min_weight_fraction_leaf=0,\n",
    "           presort=False, random_state=1, splitter='best')\n",
    "\n",
    "regr.fit(X, Y)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "import math\n",
    "\n",
    "# y_pred = scalerY.inverse_transform(y_pred.reshape(-1,1)).copy()\n",
    "# Y_test = scalerY.inverse_transform(Y_test.reshape(-1,1)).copy()\n",
    "\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred))),\n",
    "      \"R2:\", r2_score(Y_test,y_pred)\n",
    "     )\n",
    "# print(\"MSE: \",mean_squared_error((np.expm1(Y_test)), np.expm1((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error(np.expm1((Y_test)), np.expm1((y_pred)))),\n",
    "#       \"R2:\", r2_score(np.exp(Y_test),np.expm1(y_pred))\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(regr, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Actual Demand :  403926.0 \n",
      "Total Predicted Demand :  403457.9783083803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Lahore</th>\n",
       "      <th>Last_2Week_Sales</th>\n",
       "      <th>Last_Week_Sales</th>\n",
       "      <th>NEUTRAL</th>\n",
       "      <th>WEEKNO</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8.177101</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8.177101</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>8.177101</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>15.242228</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>27.888078</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3.394696</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>27.888078</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>13.839917</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>15.242228</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>15.103461</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>44.254524</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>15.242228</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>15.242228</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>24.841110</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.682478</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3.394696</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>44.254524</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     IsHoliday  Lahore  Last_2Week_Sales  Last_Week_Sales  NEUTRAL  WEEKNO  \\\n",
       "78           0       0               0.0              0.0        1      21   \n",
       "216          0       0               6.0              9.0        1      21   \n",
       "220          0       0               9.0              9.0        1      21   \n",
       "230          0       0               5.0              9.0        1      20   \n",
       "231          0       0               9.0             18.0        1      21   \n",
       "232          0       0              18.0             44.0        1      22   \n",
       "268          0       0               0.0              0.0        1      19   \n",
       "366          0       0               2.0              6.0        1      19   \n",
       "557          0       0               0.0              0.0        1      20   \n",
       "558          0       0               1.0             35.0        1      21   \n",
       "656          0       0              53.0              1.0        1      19   \n",
       "657          0       0               1.0             29.0        1      20   \n",
       "666          0       0               0.0              0.0        1      22   \n",
       "667          0       0               0.0              0.0        1      20   \n",
       "669          0       0              20.0              9.0        1      22   \n",
       "673          0       0               0.0              0.0        1      19   \n",
       "674          0       0              14.0             62.0        1      19   \n",
       "678          0       0               0.0              0.0        1      21   \n",
       "679          0       0               3.0             28.0        1      22   \n",
       "680          0       0               0.0              0.0        1      20   \n",
       "681          0       0               6.0             24.0        1      21   \n",
       "682          0       0              24.0             28.0        1      22   \n",
       "690          0       0               0.0              0.0        1      20   \n",
       "694          0       0               0.0              5.0        1      19   \n",
       "695          0       0               5.0             70.0        1      20   \n",
       "\n",
       "     Predicted  Actual  \n",
       "78    1.682478    66.0  \n",
       "216   8.177101    21.0  \n",
       "220   8.177101    11.0  \n",
       "230   8.177101    18.0  \n",
       "231  15.242228    44.0  \n",
       "232  27.888078    11.0  \n",
       "268   1.682478    11.0  \n",
       "366   3.394696    16.0  \n",
       "557   1.682478    35.0  \n",
       "558  27.888078    68.0  \n",
       "656  13.839917    29.0  \n",
       "657  15.242228    42.0  \n",
       "666   1.682478    20.0  \n",
       "667   1.682478    20.0  \n",
       "669  15.103461    13.0  \n",
       "673   1.682478    20.0  \n",
       "674  44.254524    15.0  \n",
       "678   1.682478    28.0  \n",
       "679  15.242228    78.0  \n",
       "680   1.682478    24.0  \n",
       "681  15.242228    28.0  \n",
       "682  24.841110    29.0  \n",
       "690   1.682478    11.0  \n",
       "694   3.394696    70.0  \n",
       "695  44.254524    50.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold = X_test.copy()\n",
    "\n",
    "hold['Predicted'] = y_pred\n",
    "hold['Actual'] = Y_test\n",
    "\n",
    "print('Total Actual Demand : ',hold.Actual.sum(),\n",
    "      '\\nTotal Predicted Demand : ',hold.Predicted.sum())\n",
    "\n",
    "#hold = tXs[['CATEGORY_LEVEL_3','CATEGORY_LEVEL_4','BRAND_NAME',\n",
    "#            'Last_2Week_Sales','Last_Week_Sales']].join(hold[['WEEKNO','Actual','Predicted']])\n",
    "hold[hold.Actual > 10].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold[hold.Actual == hold.Actual.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_tree(regr, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_list = Diff((X_test.SKU.unique()),X.SKU.unique())\n",
    "print(len(main_list))\n",
    "print(\"Train SKU\",len(X.SKU.unique()),\"Test SKU\",len(X_test.SKU.unique()))\n",
    "\n",
    "\n",
    "def Diff(li1, li2): \n",
    "    return (list(set(li1) - set(li2))) \n",
    "  \n",
    "# Driver Code \n",
    "li1 = [10,25, 40, 35,100,200,300,400,500] \n",
    "li2 = [10, 15, 20, 25, 30, 35] \n",
    "\n",
    "print(Diff(li2, li1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbreg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor  \n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [i for i in range(4,15)],\n",
    "    \"max_depth\": [i for i in range(2,12)],\n",
    "    \"learning_rate\": [i/20 for i in range(2,20)],\n",
    "    \"colsample_bytree\": [i/10 for i in range(3,10)],\n",
    "    \"subsample\": [i/10 for i in range(3,10)],\n",
    "    \"gamma\": [i/10.0 for i in range(1,10)],\n",
    "    'reg_alpha': [i/10 for i in range(1,10)],\n",
    "    \"min_child_weight\": [i/10 for i in range(1,10)],\n",
    "    \"silent\": [False,True]\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor() \n",
    "xgbreg = RandomizedSearchCV(xgbreg, param_distributions=params,scoring='r2',n_iter=5,verbose=4,n_jobs=4)\n",
    "xgbreg.fit(X, Y)\n",
    "\n",
    "y_pred = xgbreg.best_estimator_.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred))),\n",
    "      \"R2:\", r2_score(Y_test,y_pred)\n",
    "     )\n",
    "# print(\"MSE: \",mean_squared_error((np.exp(Y_test)), np.exp((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error(np.exp((Y_test)), np.exp((y_pred))))\n",
    "#      )\n",
    "#print (xgbreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "#set up the parameters\n",
    "plot_tree(xgbreg._Booster, num_trees=0, rankdir='LR')\n",
    "rcParams['figure.figsize'] = 80,50\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "fig.savefig('tree.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLYNOMIAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # create a Linear Regressor   \n",
    "# lin_regressor = LinearRegression()\n",
    "\n",
    "# # pass the order of your polynomial here  \n",
    "# poly = PolynomialFeatures(2)\n",
    "\n",
    "# # convert to be used further to linear regression\n",
    "# X_transform = poly.fit_transform(X.Last_Week_Sales.values.reshape(-1, 1))\n",
    "\n",
    "# # fit this to Linear Regressor\n",
    "# lin_regressor.fit(X_transform,Y.values.reshape(-1,1)) \n",
    "\n",
    "# # get the predictions\n",
    "# y_pred = lin_regressor.predict(X_test.Last_Week_Sales.values.reshape(-1, 1))\n",
    "\n",
    "# import math\n",
    "# print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "#      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch For Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = RandomizedSearchCV(xgbreg, params, n_jobs=1)  \n",
    "gs.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n",
    "# print(\"MSE: \",mean_squared_error((np.exp(Y_test)), np.exp((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error(np.exp((Y_test)), np.exp((y_pred))))\n",
    "#      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 50,\n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': 12,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'learning_rate': 0.001,\n",
    "    'feature_fraction': 0.77,\n",
    "    'bagging_fraction': 0.77,\n",
    "    'bagging_freq': 3,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 4\n",
    "}\n",
    "MAX_ROUNDS = 1000\n",
    "\n",
    "lgb_train = lgb.Dataset(X, Y)\n",
    "lgb_test = lgb.Dataset(X_test, Y_test, reference=lgb_train)\n",
    "\n",
    "gbm = lgb.train(\n",
    "       params, lgb_train, num_boost_round=MAX_ROUNDS,\n",
    "       valid_sets=lgb_test, early_stopping_rounds=50, verbose_eval=50\n",
    "   )\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred)))\n",
    "     )\n",
    "# print(\"MSE: \",mean_squared_error((np.exp(Y_test)), np.exp((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error(np.exp((Y_test)), np.exp((y_pred))))\n",
    "#      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
    "           max_features='auto', max_leaf_nodes=50,\n",
    "           min_impurity_decrease=0.001, min_impurity_split=None,\n",
    "           min_samples_leaf=20, min_samples_split=10,\n",
    "           min_weight_fraction_leaf=0.001, n_estimators=10, n_jobs=1,\n",
    "           oob_score=True, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "forest.fit(X, Y)\n",
    "\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "import math\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred))),\n",
    "      \"R2:\", r2_score(Y_test,y_pred)\n",
    "     )\n",
    "# print(\"MSE: \",mean_squared_error((np.exp(Y_test)), np.exp((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error(np.exp((Y_test)), np.exp((y_pred))))\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(4,6,2),\n",
    "                                       activation='relu',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=200,\n",
    "                                       learning_rate_init=0.0001,\n",
    "                                       alpha=0.02,\n",
    "                                       batch_size = 400,\n",
    "                                       verbose=True)\n",
    "mlp.fit(Xt, Y)\n",
    "\n",
    "y_pred = mlp.predict(Xt_test)\n",
    "import math\n",
    "\n",
    "# y_pred1 = scalerY.inverse_transform(y_pred.reshape(-1,1)).copy()\n",
    "# Y_test1 = scalerY.inverse_transform(Y_test.reshape(-1,1)).copy()\n",
    "\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred))),\n",
    "      \"R2:\", r2_score(Y_test,y_pred)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "# # sgd = linear_model.SGDRegressor(alpha=0.0001, average=False, epsilon=1, eta0=0.00001,\n",
    "# #        fit_intercept=True, l1_ratio=0.001, learning_rate='invscaling',\n",
    "# #        loss='huber', max_iter=None, n_iter=300, penalty='l2',\n",
    "# #        power_t=0.000015, random_state=None, shuffle=True, tol=None,\n",
    "# #        verbose=0, warm_start=True)\n",
    "# # sgd.fit(X, Y)\n",
    "# # y_pred = sgd.predict(X_test)\n",
    "# params = {  \n",
    "#     \"alpha\": [i/1000 for i in range(1,100)],\n",
    "#     \"epsilon\": [i/1000 for i in range(1,100)],\n",
    "#     \"eta0\": [i/1000 for i in range(1,100)],\n",
    "#     \"l1_ratio\": [i/1000 for i in range(1,100)],\n",
    "#     \"learning_rate\": ['invscaling','optimal'],\n",
    "#     \"loss\": ['huber','squared_loss'],\n",
    "#     'max_iter': [i for i in range(50,500)],\n",
    "#     \"power_t\": [i/1000 for i in range(1,100)],\n",
    "#     \"random_state\": [1001]\n",
    "# }\n",
    "\n",
    "# sgd = linear_model.SGDRegressor()\n",
    "# sgd = RandomizedSearchCV(sgd, param_distributions=params,scoring='r2',n_iter=10000,verbose=100,n_jobs=4)\n",
    "# sgd.fit(X, Y)\n",
    "\n",
    "# y_pred = sgd.best_estimator_.predict(X_test)\n",
    "\n",
    "# import math\n",
    "\n",
    "# y_pred1 = scalerY.inverse_transform(y_pred.reshape(-1,1)).copy()\n",
    "# Y_test1 = scalerY.inverse_transform(Y_test.reshape(-1,1)).copy()\n",
    "\n",
    "# print(\"MSE: \",mean_squared_error((Y_test1), (y_pred1)),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error((Y_test1), (y_pred1))),\n",
    "#       \"R2:\", r2_score(Y_test1,y_pred1)\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = X_test.copy()\n",
    "\n",
    "hold['Predicted'] = y_pred\n",
    "hold['Actual'] = Y_test\n",
    "\n",
    "print('Total Actual Demand : ',hold.Actual.sum(),\n",
    "      '\\nTotal Predicted Demand : ',hold.Predicted.sum())\n",
    "\n",
    "hold = X_test[X_test.columns].join(hold[['Actual','Predicted']])\n",
    "hold[hold.Actual > 6].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge (alpha=0.213, copy_X=True, fit_intercept=True, max_iter=500,\n",
    "      normalize=True, random_state=1, solver='lsqr', tol=0.001)\n",
    "reg.fit(X,Y)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "import math\n",
    "\n",
    "# y_pred1 = scalerY.inverse_transform(y_pred.reshape(-1,1)).copy()\n",
    "# Y_test1 = scalerY.inverse_transform(Y_test.reshape(-1,1)).copy()\n",
    "print(\"MSE: \",mean_squared_error((Y_test), (y_pred)),\n",
    "      \"RMSE: \",math.sqrt(mean_squared_error((Y_test), (y_pred))),\n",
    "      \"R2:\", r2_score(Y_test,y_pred)\n",
    "     )\n",
    "\n",
    "# print(\"MSE: \",mean_squared_error((np.expm1(Y_test)), np.expm1((y_pred))),\n",
    "#       \"RMSE: \",math.sqrt(mean_squared_error((np.expm1(Y_test)), np.expm1((y_pred)))),\n",
    "#       \"R2:\", r2_score(np.expm1(Y_test),np.expm1(y_pred))\n",
    "#      )\n",
    "\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "docs = temp['CatConcat'].head().tolist()\n",
    "\n",
    "def token_filter(token):\n",
    "    \n",
    "    if str(token).isdigit():\n",
    "        return False\n",
    "    else:\n",
    "        return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 3)\n",
    "\n",
    "filtered_tokens = []\n",
    "for doc in nlp.pipe(docs):\n",
    "    tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "    filtered_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists('./productCategoryStemmedDataFile'):\n",
    "    productCategoryStemmed = pd.read_pickle('./productCategoryStemmedDataFile', compression='infer')\n",
    "    productCategoryStemmed.columns = ['SKU','PRODUCT_STEMMED']\n",
    "    productCategoryStemmed['PRODUCT_STEMMED'] = pd.DataFrame(productCategoryStemmed)['PRODUCT_STEMMED'].apply(lambda x: x.split('-*-')[1])\n",
    "    temp = pd.merge(temp, productCategoryStemmed, how='inner')\n",
    "    print('File Found')\n",
    "    \n",
    "else:       \n",
    "    #docs = temp['CatConcat'].head(1000).values\n",
    "    def normalize(docs):\n",
    "        filtered_tokens = []\n",
    "        for doc in nlp.pipe(docs):\n",
    "            key = doc.text.split('-*-')[0]\n",
    "            #doc = doc.map(str).split('-*-')[1]\n",
    "            tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "            filtered_tokens.append((key,\" \".join(tokens)))\n",
    "        return filtered_tokens\n",
    "\n",
    "    %time test = temp['SKU'].map(str)+'-*-'+temp['CatConcat'].map(str)\n",
    "    %time productCategoryStemmed = normalize(test.drop_duplicates().tolist())\n",
    "    pd.DataFrame(productCategoryStemmed).to_pickle('./productCategoryStemmedDataFile',compression='infer', protocol=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab['game'].similarity(nlp.vocab['game'])\n",
    "from spacy.lang.en import English\n",
    "docs = temp['PRODUCT_STEMMED'].head(5).tolist()\n",
    "parser = English()\n",
    "text1 = \"I like statements that are both true and absurd.\"\n",
    "filtered_tokens = []\n",
    "def getTokens(docs):\n",
    "    for doc in nlp.pipe(docs): \n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        filtered_tokens.append(tokens)\n",
    "        #for data in doc:\n",
    "    yield filtered_tokens\n",
    "        \n",
    "#print ([data for data in getTokens(docs)])\n",
    "# tokens = parser(text1)\n",
    "# tokens = [token.orth_ for token in tokens if token_filter(token)]\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    " \n",
    "[w.lower_ for w in most_similar(nlp.vocab['cricket'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./w2vmodel not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "# WORD2VEC\n",
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            #print('arr : ',array)\n",
    "            for document in nlp.pipe(array):\n",
    "                #print('doc : ',type(document))\n",
    "                tokens = [token.lemma_ for token in document]\n",
    "                yield tokens\n",
    "                \n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=400, window=4, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model\n",
    "\n",
    "w2vec = get_word2vec(MySentences(temp['PRODUCT_STEMMED'].unique()), './w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "for doc in tokenizer.pipe(temp['PRODUCT_STEMMED'].head(), batch_size=50):\n",
    "    for token.lemma_ in doc:\n",
    "        print(token.lemma_)\n",
    "\n",
    "#ss = temp['PRODUCT_STEMMED'].head().apply(lambda x: nlp.tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danyal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('24-k', 0.9723449945449829),\n",
       " ('1k', 0.9183204174041748),\n",
       " ('18k', 0.9137002229690552),\n",
       " ('boondi', 0.9135420918464661),\n",
       " ('14-k', 0.9078975319862366),\n",
       " ('samee', 0.8863838315010071),\n",
       " ('simulat', 0.8863443732261658),\n",
       " ('rscw-5500', 0.8844240307807922),\n",
       " ('beneta', 0.8781148195266724),\n",
       " ('milka', 0.8773046731948853)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['18-k'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros`\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danyal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(X['PRODUCT_STEMMED'])\n",
    "\n",
    "mean_embedded_test = mean_embedding_vectorizer.transform(X_test['PRODUCT_STEMMED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15795"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = w2vec.wv.index2word\n",
    "wvs = w2vec.wv[words]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xx = MyTokenizer().fit_transform(X['PRODUCT_STEMMED'].head())\n",
    "dd = np.array([np.mean([w2vec.wv[w] for w in words if w in w2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0) for words in Xx]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = pd.concat([X.reset_index(drop=True),pd.DataFrame(mean_embedded).reset_index(drop=True)],axis=1)\n",
    "Xt_test = pd.concat([X_test.reset_index(drop=True),pd.DataFrame(mean_embedded_test).reset_index(drop=True)],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903442,)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
